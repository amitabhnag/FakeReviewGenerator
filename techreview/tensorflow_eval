Model Name: Char-nn with Tensorflow

GitHub Repo: https://github.com/sherjilozair/char-rnn-tensorflow

Run - 1

Time to run: 8 hours

Data Set Size: Full dataset - 70mb

Quality of output:

This sink, an option that falls bitch. The title track by playing its own freedom phase to still about being much terribly that Wall, that Das just have never really been hearing the price of full solo down the original trickles of Malkmic’s giants on Tujus Prodigy, but sorta former Homties have shoke jam by Freddie Scisn.Contemporary mutual and, it’s easy to hopefully keep wrong herself.Fall blow my own friend but simply the quicksane hip-hop, getting playfully delivered forces and clas.

No of epochs: 3

Final Train Loss: 1.4

Conclusion: The entire dataset could be trained for only 3 epochs in around 8 hours using a virtual machine with gpu in Aws. The resulting training loss is not ideal, and needs to be further optimized, by figuring out how to reduce the training time.

Run - 2

Time to run: 2 hours

Data Set Size: no special characters/reduced size dataset - 5mb

Quality of output:

apparently to produce allbazz with the occasional traditional particularly laidback drummer john really rhythm as a singersongwriter so often occasionally poorly has released its pretty as the blowfield of such a third regard summit with that much that has only released somewhere between and proven paper lyrics are lucky repetitive drumming on the plan for some sort of trail off your soul as they do in early 80s so long on is positively explicit throughout the tempo ilk veloso deadly was this flusher of be here moves and last nights is some pleasure synths incorporated as the cardighs and sandviching the or hooked up all the songs but when it becomes the consistent chord change that turns not to mention the rest of the fullestruck dc ida record matthew fury and then youll have gratuitously generated where i was born to offer the guy we have to keep things feel by past spacemen 3 greens steven mans grievous gabriels on the status to child and odd results later combined”

No of epochs: 20

Final Train Loss: 1.002

Conclusion: The reduced size dataset of around 5mb with no special characters, could be trained in around 2 hours for 20 epochs, to achieve a final train loss of around 1, which is quite decent. Hyperparameter tuning would be required to optimize it further. 
Overall, the model has potential, with training time being the major challenge.

**************************************************************************************************
Model Name: word-rnn-tensorflow

GitHub Repo: https://github.com/hunkim/word-rnn-tensorflow

Run - 1

Time to run: 1.7 hours

Data Set Size: Partial dataset - 5 mb

Quality of output: frat album soon its what is a criticism with it well have become a 75 not forget that things first purchase is angry sweet reckless and poorly some new effects days they did no problem with our careers on skylarking and radioheads other effort sparhawks points if not to say the ramifications from death cab for cutie turn their website shamelessly contained into the downtrodden elements of it the only one these songs check the trek from the tombs seeks picking a leap back to pretty long moments from its most listenable revealing new genre a photograph and good songs id sound like ever at no records ive been dreading deaf for his pisstakes and misdirection was just too good that the opening tracks here might be the show i was conceived but whereas the hattifatteners fights labels over this record at least if theres only the most weird thing in the background labeled a club done but it goes spastic back in the country and miss sunday experiments that mentioned the entire end of this reviewnow that the next time christening themselves something better toby was as often sorely bad to hear valid tracks by bringing apart the haphazard connection

No of epochs: 50

Final Train Loss: 3.8

Conclusion: This model was only possible to run with partial dataset. With full dataset of 70 MB, the model was very slow and will have taken 5.3 days to train. 

**************************************************************************************************
Model Name: keras example

GitHub Repo: https://github.com/keras-team/keras

Run - 1

Time to run: NA

Data Set Size: 70 mb

Quality of output: NA

No of epochs: NA

Final Train Loss: NA

Conclusion: It is character based as compared to word. It would have taken 60 hrs on AWS. So did not run. Quality of output for a small data set shakespere was not good too.
**************************************************************************************************
Model Name: char-rnn.pytorch

GitHub Repo: https://github.com/spro/char-rnn.pytorch/blob/master/train.py

Run - 1 with GRU model

Time to run: 20 minutes

Dataset Size: 8.6M	./Kaggle_reduced_1.txt

Quality of output: 
[17m 31s (1700 85%) 1.5630]
Whant from the little here of swirlized thing than release peeciache the universe with the such most i 

[18m 32s (1800 90%) 1.5426]
Whole as the mary me center the albums of atrhys as the unsust smoothed to head of the words time pain 

[19m 32s (1900 95%) 1.5491]
Whaltacrand listen composed may interlood is import with argues it solitar baby of the album content c 

[20m 33s (2000 100%) 1.5147]
Whance
 that a delain course the rock by space the
 piano is experiment that her be a prince you naudi 

No of epochs: 2000

Final Train Loss: 1.5147

Conclusion: this model might be very simple so the output quality is not optimal. However, very quick and simple to train.


